---
layout: archive
title: ""
excerpt: ""
author_profile: true
permalink: /research/
---

## Research

[Google Scholar Page](https://scholar.google.com/citations?hl=en&user=FO90FgMAAAAJ&view_op=list_works)

**Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind**  
Swarnadeep Saha, Peter Hase, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2306.09299.pdf) [[code]](https://github.com/swarnaHub/ExplanationIntervention)

**Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects**  
Zhuofan Ying, Peter Hase, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2306.05963.pdf) [[code]](https://github.com/zfying/AdaptiveContext)

**Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models**  
Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2301.04213.pdf) [[code]](https://github.com/google/belief-localization)

**Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees**  
Swarnadeep Saha, Shiyue Zhang, Peter Hase, Mohit Bansal  
*ICLR 2023.* [[pdf]](https://arxiv.org/pdf/2209.10492.pdf) [[code]](https://github.com/swarnaHub/SummarizationPrograms)

**GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models**  
Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal  
*EACL 2023.* [[pdf]](https://arxiv.org/pdf/2203.07281.pdf) [[code]](https://github.com/archiki/GrIPS)

**Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs**  
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer  
*EACL 2023.* [[pdf]](https://arxiv.org/pdf/2111.13654.pdf) [[code]](https://github.com/peterbhase/SLAG-Belief-Updating)

**Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations**  
Swarnadeep Saha, Peter Hase, Nazneen Rajani, Mohit Bansal  
*EMNLP 2022.* [[pdf]](https://arxiv.org/pdf/2211.07517.pdf) [[code]](https://github.com/swarnaHub/ExplanationHardness)

**VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives**  
Zhuofan Ying,* Peter Hase,* Mohit Bansal  
*NeurIPS 2022.* [[pdf]](https://arxiv.org/pdf/2206.11212.pdf) [[code]](https://github.com/zfying/visfis)

**When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data**  
Peter Hase, Mohit Bansal  
*ACL 2022 Workshop on Natural Language Supervision.* [[pdf v2]](https://peterbhase.github.io/files/when-expl-help-LNLS-ACL2022.pdf) [[pdf v1]](https://arxiv.org/pdf/2102.02201.pdf) [[code]](https://github.com/peterbhase/ExplanationRoles)

**Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions**  
Prateek Yadav, Peter Hase, Mohit Bansal  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/2111.01235.pdf) [[code]](https://github.com/prateeky2806/EMC-COLS-recourse)  

**The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations**  
Peter Hase, Harry Xie, Mohit Bansal  
*NeurIPS 2021.* [[pdf]](https://arxiv.org/pdf/2106.00786.pdf) [[code]](https://github.com/peterbhase/ExplanationSearch)  

**FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging**  
Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, Caiming Xiong  
*EMNLP 2021.* [[pdf]](https://arxiv.org/pdf/2012.15781.pdf) [[code]](https://github.com/salesforce/fast-influence-functions)  

**Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?**  
Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal  
*Findings of EMNLP.* [[pdf]](https://arxiv.org/pdf/2010.04119.pdf) [[code]](https://github.com/peterbhase/LAS-NL-Explanations)  

**Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?**  
Peter Hase, Mohit Bansal  
*ACL 2020.* [[pdf]](https://arxiv.org/pdf/2005.01831.pdf) [[code]](https://github.com/peterbhase/InterpretableNLP-ACL2020)  

**Interpretable Image Recognition with Hierarchical Prototypes**  
Peter Hase, Chaofan Chen, Oscar Li, Cynthia Rudin  
*AAAI-HCOMP 2019.* [[pdf]](https://arxiv.org/pdf/1906.10651.pdf) [[code]](https://github.com/peterbhase/interpretable-image)  

**Shall I Compare Thee to a Machine-Written Sonnet? An Approach to Algorithmic Sonnet Generation**  
John Benhardt,* Peter Hase,* Liuyi Zhu,* Cynthia Rudin  
*Preprint on arXiv.* [[pdf]](https://arxiv.org/pdf/1811.05067.pdf) [[code]](https://github.com/peterbhase/poetry-generation)  



