---
layout: archive
title: ""
excerpt: ""
author_profile: true
permalink: /research/
---

# Papers + Code

Papers are roughly grouped by topic below. For a full list, see my [Google Scholar Page](https://scholar.google.com/citations?hl=en&user=FO90FgMAAAAJ&view_op=list_works&sortby=pubdate).

## Interpretability

**System-1.x: Learning to Balance Fast and Slow Planning with Language Models**  
Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, Mohit Bansal  
_Preprint on arXiv._ [[pdf](https://arxiv.org/pdf/2407.14414)] [[code](https://github.com/swarnaHub/System-1.x)]

**Foundational Challenges in Assuring Alignment and Safety of Large Language Models** (Sec. 3.4)  
Usman Anwar and 37 others including Peter Hase  
_TMLR 2024._ [[pdf](https://arxiv.org/pdf/2404.09932)]

**Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models**  
Peter Hase, Mohit Bansal, Been Kim, Asma Ghandeharioun  
*NeurIPS 2023 (Spotlight).* [[pdf]](https://arxiv.org/pdf/2301.04213.pdf) [[code]](https://github.com/google/belief-localization)

**Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects**  
Zhuofan Ying, Peter Hase, Mohit Bansal  
*NeurIPS 2023.* [[pdf]](https://arxiv.org/pdf/2306.05963.pdf) [[code]](https://github.com/zfying/AdaptiveContext)

**Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees**  
Swarnadeep Saha, Shiyue Zhang, Peter Hase, Mohit Bansal  
*ICLR 2023.* [[pdf]](https://arxiv.org/pdf/2209.10492.pdf) [[code]](https://github.com/swarnaHub/SummarizationPrograms)

**VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives**  
Zhuofan Ying,* Peter Hase,* Mohit Bansal  
*NeurIPS 2022.* [[pdf]](https://arxiv.org/pdf/2206.11212.pdf) [[code]](https://github.com/zfying/visfis)

**Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations**  
Swarnadeep Saha, Peter Hase, Nazneen Rajani, Mohit Bansal  
*EMNLP 2022.* [[pdf]](https://arxiv.org/pdf/2211.07517.pdf) [[code]](https://github.com/swarnaHub/ExplanationHardness)

**When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data**  
Peter Hase, Mohit Bansal  
*ACL 2022 Workshop on Natural Language Supervision (Spotlight).* [[pdf v2]](https://peterbhase.github.io/files/when-expl-help-LNLS-ACL2022.pdf) [[pdf v1]](https://arxiv.org/pdf/2102.02201.pdf) [[code]](https://github.com/peterbhase/ExplanationRoles)

**The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations**  
Peter Hase, Harry Xie, Mohit Bansal  
*NeurIPS 2021.* [[pdf]](https://arxiv.org/pdf/2106.00786.pdf) [[code]](https://github.com/peterbhase/ExplanationSearch)  

**FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging**  
Han Guo, Nazneen Fatema Rajani, Peter Hase, Mohit Bansal, Caiming Xiong  
*EMNLP 2021.* [[pdf]](https://arxiv.org/pdf/2012.15781.pdf) [[code]](https://github.com/salesforce/fast-influence-functions)  

**Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?**  
Peter Hase, Mohit Bansal  
*ACL 2020.* [[pdf]](https://arxiv.org/pdf/2005.01831.pdf) [[code]](https://github.com/peterbhase/InterpretableNLP-ACL2020)  

**Interpretable Image Recognition with Hierarchical Prototypes**  
Peter Hase, Chaofan Chen, Oscar Li, Cynthia Rudin  
*AAAI-HCOMP 2019.* [[pdf]](https://arxiv.org/pdf/1906.10651.pdf) [[code]](https://github.com/peterbhase/interpretable-image)  

**Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?**  
Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal  
*Findings of EMNLP 2020.* [[pdf]](https://arxiv.org/pdf/2010.04119.pdf) [[code]](https://github.com/peterbhase/LAS-NL-Explanations)  

## Model Editing & Unlearning

**Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?**  
Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal  
*TMLR 2024.* [[pdf](https://arxiv.org/pdf/2406.19354)] [[code](https://github.com/peterbhase/LLM-belief-revision)]

**Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation**  
Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, Mohit Bansal  
*TMLR 2024.* [[pdf](https://openreview.net/pdf?id=YcnjgKbZQS)] [[code](https://github.com/Vaidehi99/UnLOK-VQA)]

**Rethinking Machine Unlearning for Large Language Models**  
Sijia Liu, Yuanshun Yao, et al. including Peter Hase  
*Nature Machine Intelligence.* [[pdf]](https://arxiv.org/pdf/2402.08787.pdf)  

**Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks**    
Vaidehi Patil,* Peter Hase,* Mohit Bansal  
*ICLR 2024 (Spotlight).* [[pdf]](https://arxiv.org/pdf/2309.17410.pdf) [[code]](https://github.com/Vaidehi99/InfoDeletionAttacks)

**Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs**  
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer  
*EACL 2023.* [[pdf]](https://arxiv.org/pdf/2111.13654.pdf) [[code]](https://github.com/peterbhase/SLAG-Belief-Updating)

## Scalable Oversight

**The Unreasonable Effectiveness of Easy Training Data for Hard Tasks**  
Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe  
*ACL 2024.* [[pdf]](https://arxiv.org/pdf/2401.06751.pdf) [[code]](https://github.com/allenai/easy-to-hard-generalization)

**Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization**  
Swarnadeep Saha, Peter Hase, Mohit Bansal  
*NeurIPS 2023.* [[pdf]](https://arxiv.org/pdf/2306.09299.pdf) [[code]](https://github.com/swarnaHub/ExplanationIntervention)

## Additional Topics  

**Teaching Models to Balance Resisting and Accepting Persuasion**  
Elias Stengel-Eskin, Peter Hase, and Mohit Bansal  
_Preprint on arXiv._ [[pdf](https://arxiv.org/pdf/2410.14596)] [[code](https://github.com/esteng/persuasion_balanced_training)]

**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**  
Elias Stengel-Eskin, Peter Hase, and Mohit Bansal  
_NeurIPS 2024._ [[pdf](https://arxiv.org/pdf/2405.21028)] [[code](https://github.com/esteng/pragmatic_calibration)]

**Are Language Models Rational? The Case of Coherence Norms and Belief Revision**  
Thomas Hofweber, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal  
_Preprint on arXiv._ [[pdf](https://arxiv.org/pdf/2406.03442)]

**INSPIRE: A Framework for Integrating Individual User Preferences in Recourse**  
Prateek Yadav, Peter Hase, Mohit Bansal  
*TMLR 2024.* [[pdf]](https://openreview.net/pdf?id=6yzIuqKGnq) [[code]](https://github.com/prateeky2806/EMC-COLS-recourse)  

**Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback**  
Stephen Casper, Xander Davies, et al. including Peter Hase  
*TMLR 2023 (Outstanding Paper Finalist).* [[pdf]](https://openreview.net/pdf?id=bx24KpJ4Eb)  

**GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models**  
Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal  
*EACL 2023.* [[pdf]](https://arxiv.org/pdf/2203.07281.pdf) [[code]](https://github.com/archiki/GrIPS)


