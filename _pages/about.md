---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!---
<p align="center">
  <img src="https://github.com/peterbhase/peterbhase.github.io/blob/master/images/s2.jpg?raw=True" alt="Photo" style="width: 300px;"/> 
</p>
-->

## About Me

I am an AI researcher currently doing a residency at Anthropic. Before this, I completed my PhD at the University of North Carolina at Chapel Hill, where I was advised by Mohit Bansal. My work at UNC was supported by a Google PhD Fellowship and a Royster Fellowship.

My research focuses on AI safety and NLP. Below are some of the main areas I am interested in:
1. Interpretability
2. Model Editing & Unlearning
3. Scalable Oversight

Broadly, I am interested in explaining and controlling the behavior of machine learning models. I see language models as a good object of study since we lack complete explanations for their behavior and human language provides a rich means of interaction with models. I find work on clarifying concepts and developing strong evaluation procedures especially valuable.

*Email:* peter@cs.unc.edu

[Faculty Package](https://peterbhase.github.io/files/faculty_package.pdf)

[Google Scholar Page](https://scholar.google.com/citations?user=FO90FgMAAAAJ&hl=en)

## News
* 2024 - Work on Open Problems and Fundamental Limitations of RLHF designated as an [Outstanding Paper Finalist](https://x.com/TmlrOrg/status/1869867719937401064) in TMLR
* 2024 - Two papers accepted to TMLR on (1) [fundamental problems in model editing](https://arxiv.org/pdf/2406.19354) and (2) [unlearning for multimodal models](https://openreview.net/pdf?id=YcnjgKbZQS)
* 2024 - Invited talk at TTIC's Young Researcher Seminar Series, "AI Safety Through Interpretable and Controllable Language Models" [[slides]](https://peterbhase.github.io/files/TTIC-talk.pdf)
* 2024 - New paper on [training LLMs to be persuadable only when appropriate](https://arxiv.org/pdf/2410.14596)
* 2024 - Paper accepted to NeurIPS on [calibrating LLMs' linguistic expressions of confidence](https://arxiv.org/pdf/2405.21028)
* 2024 - I will serve as a Senior Area Chair for ACL 2025 in the Interpretability and Analysis of Models for NLP track  
* 2024 - I am starting a residency at Anthropic! I will be working with [Sam Bowman](https://cims.nyu.edu/~sbowman/) on topics in AI safety. 
* 2024 - We have several new papers on (1) [controlling how much LLMs verbalize vs. internalize their reasoning](https://arxiv.org/pdf/2407.14414), (2), [calibrating explicit and implicit confidence markers in LLM outputs](https://arxiv.org/pdf/2405.21028), and (3) [defining a philosophical basis for epistemic rationality in LLMs](https://arxiv.org/pdf/2406.03442). 
* 2024 - My last PhD paper is out! "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?" [[pdf]](https://arxiv.org/pdf/2406.19354) [[code]](https://github.com/peterbhase/LLM-belief-revision)
* 2024 - I graduated! My thesis was on "[Interpretable and Controllable Language Models](https://peterbhase.github.io/files/hase_thesis.pdf)", and you can watch my defense [here](https://www.youtube.com/watch?v=e0kIoAMqAEg&feature=youtu.be). I have to thank a lot of people for this, and hopefully most of them are mentioned in [these acknowledgments](https://peterbhase.github.io/files/hase_thesis.pdf#page=3). 
* 2024 - Invited talk at Stanford [NLP Seminar](https://nlp.stanford.edu/seminar/) on "Controlling and Editing Knowledge in Large Language Models" [[slides]](https://peterbhase.github.io/files/Stanford_Talk.pdf)  
* 2024 - Invited talks at OpenAI and [CHAI](https://humancompatible.ai/) (UC Berkeley) on "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks
" [[slides]](https://peterbhase.github.io/files/CHAI_Presentation.pdf)  
* 2024 - New paper out! "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks" [[pdf]](https://arxiv.org/pdf/2401.06751.pdf) [[code]](https://github.com/allenai/easy-to-hard-generalization)
* 2024 - Paper accepted to ICLR with a spotlight: "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks" [[pdf]](https://arxiv.org/pdf/2309.17410.pdf) [[code]](https://github.com/Vaidehi99/InfoDeletionAttacks)  
* 2023 - Serving as an Area Chair for EACL 2024 in the Interpretability and Analysis of Models for NLP track 
* 2023 - New paper out! "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks" [[pdf]](https://arxiv.org/pdf/2309.17410.pdf) [[code]](https://github.com/Vaidehi99/InfoDeletionAttacks)  
* 2023 - Three papers accepted to NeurIPS 2023! Our work on (1) [localization and model editing](https://arxiv.org/pdf/2301.04213.pdf), (2) [mechanistic interpretability for vision models](https://arxiv.org/pdf/2306.05963.pdf), and (3) [LMs explaining tasks to weaker agents (teaching)](https://arxiv.org/pdf/2306.09299.pdf).
* 2023 - Named an [Outstanding Area Chair](https://2023.aclweb.org/program/best_reviewers/) at ACL 2023 (1-1.5% of the pool of reviewers and chairs)   
* 2023 - New paper out! "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind" [[pdf]](https://arxiv.org/pdf/2306.09299.pdf) [[code]](https://github.com/swarnaHub/ExplanationIntervention)
* 2023 - New paper out! "Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects" [[pdf]](https://arxiv.org/pdf/2306.05963.pdf) [[code]](https://github.com/zfying/AdaptiveContext)  
* 2023 - Started summer internship at AI2! Supervised by [Sarah Wiegreffe](https://sarahwie.github.io/) and [Peter Clark](https://allenai.org/team/peterc)  
* 2023 - New paper out! "Does Localization Inform Editing? Surprising Differences in
Causality-Based Localization vs. Knowledge Editing in Language Models" [[pdf]](https://arxiv.org/pdf/2301.04213.pdf) [[code]](https://github.com/google/belief-localization)  
* 2022 - Serving as an Area Chair for ACL 2023 in the Interpretability and Analysis of Models for NLP track  
* 2022 - Serving as an Area Chair for the AAAI 2023 Workshop on Representation learning for Responsible Human-Centric AI
* 2022 - Work accepted to EMNLP 2022: "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations" [[pdf]](https://arxiv.org/pdf/2211.07517.pdf)  
* 2022 - Work accepted to NeurIPS 2022: "VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives" [[pdf]](https://arxiv.org/pdf/2206.11212.pdf)  
* 2022 - Serving as an Area Chair for EMNLP 2022 in the Interpretability, Interactivity and Analysis of Models for NLP track
* 2022 - Started summer internship at Google Research! Supervised by [Asma Ghandeharioun](https://web.media.mit.edu/~asma_gh/) and [Been Kim](https://beenkim.github.io/)
* 2022 - Invited talk at the University of Oxford on Explainable Machine Learning in NLP
* 2022 - Paper accepted to ACL 2022 Workshop on [Natural Language Supervision](https://sites.google.com/princeton.edu/nl-supervision)! "When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data" [[pdf]](https://peterbhase.github.io/files/when-expl-help-LNLS-ACL2022.pdf) [[code]](https://github.com/peterbhase/ExplanationRoles)
* 2022 - Invited talk at [NEC Laboratories Europe](https://www.neclab.eu/), on Explainable Machine Learning in NLP
* 2022 - Invited talk at the [National Institute for Standards and Technology](https://www.nist.gov/), on [Evaluating Explainable AI](https://arxiv.org/abs/2005.01831)
* 2022 - Invited talk at the [Allen Institute for AI](https://allenai.org/), on [Detecting, Updating, and Visualizing Language Model Beliefs](https://arxiv.org/abs/2111.13654)
* 2022 - Invited talk at [Uber AI](https://www.uber.com/us/en/uberai/), on [The OOD Problem and Search Methods in Explainable ML](https://arxiv.org/abs/2106.00786)
* 2021 - New preprint on arxiv! "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs" [[pdf]](https://arxiv.org/abs/2111.13654) [[code]](https://github.com/peterbhase/SLAG-Belief-Updating)
* 2021 - Paper accepted to NeurIPS 2021! "The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations" [[pdf]](https://arxiv.org/abs/2106.00786) [[code]](https://github.com/peterbhase/ExplanationSearch)
* 2021 - Awarded a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/) for Natural Language Processing! 
* 2021 - Invited talk at [CHAI](https://humancompatible.ai/), UC Berkeley, on [Evaluating Explainable AI](https://arxiv.org/abs/2005.01831)
* 2021 - Paper accepted to EMNLP 2021: "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging" [[pdf]](https://arxiv.org/abs/2012.15781) [[code]](https://github.com/salesforce/fast-influence-functions)
* 2021 - Named as an [outstanding reviewer](https://2021.aclweb.org/blog/reviewer-list/) for ACL-IJCNLP 2021
* 2021 - New paper on arxiv! "Search Methods for Sufficient, Socially-Aligned Feature Importance Explanations with In-Distribution Counterfactuals" [[pdf]](https://arxiv.org/abs/2106.00786) [[code]](https://github.com/peterbhase/ExplanationSearch)
* 2021 - Started summer internship at FAIR, supervised by [Srini Iyer](http://sriniiyer.github.io/). 
* 2021 - New blog post on the Alignment Forum: "Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers" [[link]](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries)
* 2021 - New preprint on arxiv: "When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data" [[pdf]](https://arxiv.org/abs/2102.02201) [[code]](https://github.com/peterbhase/ExplanationRoles)
* 2020 - New preprint on arxiv! "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging" [[pdf]](https://arxiv.org/abs/2012.15781) [[code]](https://github.com/salesforce/fast-influence-functions)
* 2020 - Recognized as an [Outstanding Reviewer](https://www.aclweb.org/anthology/2020.emnlp-main.0.pdf#page=29) for EMNLP 2020
* 2020 - Paper accepted into [Findings of EMNLP](https://2020.emnlp.org/), "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?" [[pdf]](https://arxiv.org/abs/2010.04119) [[code]](https://github.com/peterbhase/LAS-NL-Explanations)
* 2020 - Paper accepted into [ACL 2020](https://acl2020.org/), "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?"  [[pdf]](https://arxiv.org/abs/2005.01831) [[code]](https://github.com/peterbhase/InterpretableNLP-ACL2020)
* 2019 - Paper accepted into [AAAI-HCOMP 2019](https://www.humancomputation.com/), "Interpretable Image Recognition with Hierarchical Prototypes" [[pdf]](https://arxiv.org/abs/1906.10651) [[code]](https://github.com/peterbhase/interpretable-image)
* 2019 - Joined the [UNC NLP](https://nlp.cs.unc.edu/) lab
* 2019 - Graduated with a B.S. from the Department of Statistical Science at Duke University
* 2019 - Awarded a [Royster PhD Fellowship](https://gradschool.unc.edu/funding/gradschool/royster/membership.html) from UNC Chapel Hill

