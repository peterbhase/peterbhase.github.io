---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!---
<p align="center">
  <img src="https://github.com/peterbhase/peterbhase.github.io/blob/master/images/s2.jpg?raw=True" alt="Photo" style="width: 300px;"/> 
</p>
-->

## About Me

I am a fifth-year PhD student in the [UNC-NLP](https://nlp.cs.unc.edu/) lab at the University of North Carolina at Chapel Hill, where I am advised by [Mohit Bansal](http://www.cs.unc.edu/~mbansal/). My work at UNC is supported by a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/) and previously by a [Royster Fellowship](https://gradschool.unc.edu/funding/gradschool/royster/membership.html). Before this, I graduated with a bachelor's degree from Duke University, where my thesis advisor was [Cynthia Rudin](https://users.cs.duke.edu/~cynthia/). At Duke I was supported by a [Trinity Scholarship](https://ousf.duke.edu/merit-scholarships/trinity-scholars-program/). 

My research interests center on interpretable machine learning and natural language processing. Below are some of the main problems Iâ€™ve worked on (publications [here](https://peterbhase.github.io/research/)):
1. Mechanistic Interpretability
2. Neural Module Programs / Process Supervision
3. Natural Language Explanations
4. Model Editing
5. XAI Methods & Evaluation
6. Algorithmic Recourse

Broadly, I am interested in explaining model behavior and improving model safety. I see language models as a good object of study since we lack complete explanations for their behavior and human language provides a rich means of interaction with models. I find work on clarifying concepts and developing strong evaluation procedures especially valuable.

*Email:* peter@cs.unc.edu

## News
* 2023 - Serving as an Area Chair for EACL 2024 in the Interpretability and Analysis of Models for NLP track 
* 2023 - New paper out! "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks" [[pdf]](https://arxiv.org/pdf/2309.17410.pdf) [[code]](https://github.com/Vaidehi99/InfoDeletionAttacks)  
* 2023 - Three papers accepted to NeurIPS 2023! Our work on (1) [localization and model editing](https://arxiv.org/pdf/2301.04213.pdf), (2) [mechanistic interpretability for vision models](https://arxiv.org/pdf/2306.05963.pdf), and (3) [LMs explaining tasks to weaker agents (teaching)](https://arxiv.org/pdf/2306.09299.pdf).
* 2023 - Named an [Outstanding Area Chair](https://2023.aclweb.org/program/best_reviewers/) at ACL 2023 (1-1.5% of the pool of reviewers and chairs)   
* 2023 - New paper out! "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind" [[pdf]](https://arxiv.org/pdf/2306.09299.pdf) [[code]](https://github.com/swarnaHub/ExplanationIntervention)
* 2023 - New paper out! "Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects" [[pdf]](https://arxiv.org/pdf/2306.05963.pdf) [[code]](https://github.com/zfying/AdaptiveContext)  
* 2023 - Started summer internship at AI2! Supervised by [Sarah Wiegreffe](https://sarahwie.github.io/) and [Peter Clark](https://allenai.org/team/peterc)  
* 2023 - New paper out! "Does Localization Inform Editing? Surprising Differences in
Causality-Based Localization vs. Knowledge Editing in Language Models" [[pdf]](https://arxiv.org/pdf/2301.04213.pdf) [[code]](https://github.com/google/belief-localization)  
* 2022 - Serving as an Area Chair for ACL 2023 in the Interpretability and Analysis of Models for NLP track  
* 2022 - Serving as an Area Chair for the AAAI 2023 Workshop on Representation learning for Responsible Human-Centric AI
* 2022 - Work accepted to EMNLP 2022: "Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations" [[pdf]](https://arxiv.org/pdf/2211.07517.pdf)  
* 2022 - Work accepted to NeurIPS 2022: "VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives" [[pdf]](https://arxiv.org/pdf/2206.11212.pdf)  
* 2022 - Serving as an Area Chair for EMNLP 2022 in the Interpretability, Interactivity and Analysis of Models for NLP track
* 2022 - Started summer internship at Google Research! Supervised by [Asma Ghandeharioun](https://web.media.mit.edu/~asma_gh/) and [Been Kim](https://beenkim.github.io/)
* 2022 - Invited talk at the University of Oxford on Explainable Machine Learning in NLP
* 2022 - Paper accepted to ACL 2022 Workshop on [Natural Language Supervision](https://sites.google.com/princeton.edu/nl-supervision)! "When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data" [[pdf]](https://peterbhase.github.io/files/when-expl-help-LNLS-ACL2022.pdf) [[code]](https://github.com/peterbhase/ExplanationRoles)
* 2022 - Invited talk at [NEC Laboratories Europe](https://www.neclab.eu/), on Explainable Machine Learning in NLP
* 2022 - Invited talk at the [National Institute for Standards and Technology](https://www.nist.gov/), on [Evaluating Explainable AI](https://arxiv.org/abs/2005.01831)
* 2022 - Invited talk at the [Allen Institute for AI](https://allenai.org/), on [Detecting, Updating, and Visualizing Language Model Beliefs](https://arxiv.org/abs/2111.13654)
* 2022 - Invited talk at [Uber AI](https://www.uber.com/us/en/uberai/), on [The OOD Problem and Search Methods in Explainable ML](https://arxiv.org/abs/2106.00786)
* 2021 - New preprint on arxiv! "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs" [[pdf]](https://arxiv.org/abs/2111.13654) [[code]](https://github.com/peterbhase/SLAG-Belief-Updating)
* 2021 - Paper accepted to NeurIPS 2021! "The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations" [[pdf]](https://arxiv.org/abs/2106.00786) [[code]](https://github.com/peterbhase/ExplanationSearch)
* 2021 - Awarded a [Google PhD Fellowship](https://research.google/outreach/phd-fellowship/) for Natural Language Processing! 
* 2021 - Invited talk at [CHAI](https://humancompatible.ai/), UC Berkeley, on [Evaluating Explainable AI](https://arxiv.org/abs/2005.01831)
* 2021 - Paper accepted to EMNLP 2021: "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging" [[pdf]](https://arxiv.org/abs/2012.15781) [[code]](https://github.com/salesforce/fast-influence-functions)
* 2021 - Named as an [outstanding reviewer](https://2021.aclweb.org/blog/reviewer-list/) for ACL-IJCNLP 2021
* 2021 - New paper on arxiv! "Search Methods for Sufficient, Socially-Aligned Feature Importance Explanations with In-Distribution Counterfactuals" [[pdf]](https://arxiv.org/abs/2106.00786) [[code]](https://github.com/peterbhase/ExplanationSearch)
* 2021 - Started summer internship at FAIR, supervised by [Srini Iyer](http://sriniiyer.github.io/). 
* 2021 - New blog post on the Alignment Forum: "Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers" [[link]](https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries)
* 2021 - New preprint on arxiv: "When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data" [[pdf]](https://arxiv.org/abs/2102.02201) [[code]](https://github.com/peterbhase/ExplanationRoles)
* 2020 - New preprint on arxiv! "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging" [[pdf]](https://arxiv.org/abs/2012.15781) [[code]](https://github.com/salesforce/fast-influence-functions)
* 2020 - Recognized as an [Outstanding Reviewer](https://www.aclweb.org/anthology/2020.emnlp-main.0.pdf#page=29) for EMNLP 2020
* 2020 - Paper accepted into [Findings of EMNLP](https://2020.emnlp.org/), "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?" [[pdf]](https://arxiv.org/abs/2010.04119) [[code]](https://github.com/peterbhase/LAS-NL-Explanations)
* 2020 - Paper accepted into [ACL 2020](https://acl2020.org/), "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?"  [[pdf]](https://arxiv.org/abs/2005.01831) [[code]](https://github.com/peterbhase/InterpretableNLP-ACL2020)
* 2019 - Paper accepted into [AAAI-HCOMP 2019](https://www.humancomputation.com/), "Interpretable Image Recognition with Hierarchical Prototypes" [[pdf]](https://arxiv.org/abs/1906.10651) [[code]](https://github.com/peterbhase/interpretable-image)
* 2019 - Joined the [UNC NLP](https://nlp.cs.unc.edu/) lab
* 2019 - Graduated with a B.S. from the Department of Statistical Science at Duke University
* 2019 - Awarded a [Royster PhD Fellowship](https://gradschool.unc.edu/funding/gradschool/royster/membership.html) from UNC Chapel Hill

